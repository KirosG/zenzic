{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Does the Federal Government Close for Forecasted Snow or the Hype for Snow? \n",
    "\n",
    "The Office of Personnel Management within the federal government has the [almighty power](https://www.opm.gov/policy-data-oversight/snow-dismissal-procedures/current-status/) of declaring snow days. **Do they base this decision entirely on weather forecasts, or do they fall victim to the hype that surrounds possible snow days?**\n",
    "\n",
    "To answer this question, I gathered every single snow day OPM has declared (scraper [here](https://github.com/josephofiowa/zenzic/blob/master/federal-snow/fedsnow-scraper.py)). I also used the Dark Sky API to gather historical weather forecasts -- not historical *actual* weather, but what forecast was available to OPM at the time of their decision ([script](https://darksky.net/dev/) and [data](https://github.com/josephofiowa/zenzic/blob/master/federal-snow/forecasts.csv)). Finally, I downloaded the last five years of [Google Trends](https://trends.google.com/trends/explore?geo=US-DC&q=snow) search data for searches of \"snow\" within DC ([data](https://github.com/josephofiowa/zenzic/blob/master/federal-snow/google-trends-dc-snow.csv)).\n",
    "\n",
    "I hope you enjoy the analysis that follows just as much as I did completing it. It's commented and everything is open source.\n",
    "\n",
    "Keep up with the latest data science trends, cool analyses like this, and more via [Entropy](https://www.josephofiowa.com/entropy/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# display plots in the notebook\n",
    "%matplotlib inline\n",
    "\n",
    "# increase default figure and font sizes for easier viewing\n",
    "plt.rcParams['figure.figsize'] = (14, 6)\n",
    "plt.rcParams['font.size'] = 14\n",
    "\n",
    "# always be stylish\n",
    "plt.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OPM Federal Closure Data\n",
    "\n",
    "Read in the data, clean it a bit, make a few plots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "opm = pd.read_csv('./opm_snow_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# rid unnecessary column...\n",
    "opm.drop('Unnamed: 0', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>year</th>\n",
       "      <th>date</th>\n",
       "      <th>notice</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2017</td>\n",
       "      <td>March 15, 2017</td>\n",
       "      <td>Open</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2017</td>\n",
       "      <td>March 14, 2017</td>\n",
       "      <td>Open - 3 hours Delayed Arrival - With Option f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2016</td>\n",
       "      <td>March 16, 2016</td>\n",
       "      <td>Open with Option for Unscheduled Leave or Unsc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2016</td>\n",
       "      <td>February 16, 2016</td>\n",
       "      <td>Open - 3 hours Delayed Arrival - With Option f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2016</td>\n",
       "      <td>February 9, 2016</td>\n",
       "      <td>Open with Option for Unscheduled Leave or Unsc...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   year               date                                             notice\n",
       "0  2017     March 15, 2017                                               Open\n",
       "1  2017     March 14, 2017  Open - 3 hours Delayed Arrival - With Option f...\n",
       "2  2016     March 16, 2016  Open with Option for Unscheduled Leave or Unsc...\n",
       "3  2016  February 16, 2016  Open - 3 hours Delayed Arrival - With Option f...\n",
       "4  2016   February 9, 2016  Open with Option for Unscheduled Leave or Unsc..."
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check 'er out\n",
    "opm.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean the date values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0          March 15, 2017\n",
       "1          March 14, 2017\n",
       "2          March 16, 2016\n",
       "3       February 16, 2016\n",
       "4        February 9, 2016\n",
       "5        January 29, 2016\n",
       "6        January 28, 2016\n",
       "7        January 27, 2016\n",
       "8        January 26, 2016\n",
       "9        January 25, 2016\n",
       "10       January 22, 2016\n",
       "11       January 22, 2016\n",
       "12          March 6, 2015\n",
       "13          March 5, 2015\n",
       "14          March 3, 2015\n",
       "15          March 2, 2015\n",
       "16      February 26, 2015\n",
       "17      February 18, 2015\n",
       "18      February 17, 2015\n",
       "19       January 27, 2015\n",
       "20       January 26, 2015\n",
       "21       January 21, 2015\n",
       "22       January 14, 2015\n",
       "23       January 12, 2015\n",
       "24        January 6, 2015\n",
       "25         March 17, 2014\n",
       "26          March 4, 2014\n",
       "27          March 3, 2014\n",
       "28      February 26, 2014\n",
       "29      February 14, 2014\n",
       "              ...        \n",
       "117          September 18\n",
       "118              March 19\n",
       "119              March 18\n",
       "120           February 28\n",
       "121           February 27\n",
       "122           February 20\n",
       "123           February 19\n",
       "124           February 18\n",
       "125            February 7\n",
       "126            January 17\n",
       "127           December 12\n",
       "128            December 6\n",
       "129            December 5\n",
       "130            December 4\n",
       "131     February 23, 2001\n",
       "132     February 22, 2001\n",
       "133              April 17\n",
       "134     February 18, 2000\n",
       "135      January 31, 2000\n",
       "136            January 26\n",
       "137      January 25, 2000\n",
       "138      January 20, 2000\n",
       "139       October 1, 1999\n",
       "140    September 16, 1999\n",
       "141        March 10, 1999\n",
       "142         March 9, 1999\n",
       "143      January 15, 1999\n",
       "144       January 8, 1999\n",
       "145           December 24\n",
       "146      January 15, 1998\n",
       "Name: date, dtype: object"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# I remember scraping a messy date column (hence the \"year\" and \"date\" columns). Indeed, this will need some cleaning.\n",
    "opm.date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# function to clean dates\n",
    "# if there's a comma, convert to datetime and return\n",
    "# if there's not a comma, grab month/day from something like 'January 27 (PM)' and concatenate with year. Return result\n",
    "def convert_date(year, date):\n",
    "    if ',' in date:\n",
    "        return(pd.to_datetime(date))\n",
    "    else:\n",
    "        item = date.split(' (')[0]\n",
    "        return(pd.to_datetime(str(item) + ', ' + str(year)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# use function\n",
    "opm['clean_date'] = opm.apply(lambda row: convert_date(row['year'], row['date']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1998-01-15 00:00:00\n",
      "2026-12-25 20:03:00\n"
     ]
    }
   ],
   "source": [
    "# check...\n",
    "print(opm.clean_date.min())\n",
    "print(opm.clean_date.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>year</th>\n",
       "      <th>date</th>\n",
       "      <th>notice</th>\n",
       "      <th>clean_date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2017</td>\n",
       "      <td>March 15, 2017</td>\n",
       "      <td>Open</td>\n",
       "      <td>2017-03-15 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2017</td>\n",
       "      <td>March 14, 2017</td>\n",
       "      <td>Open - 3 hours Delayed Arrival - With Option f...</td>\n",
       "      <td>2017-03-14 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115</th>\n",
       "      <td>2003</td>\n",
       "      <td>December 25 and 26</td>\n",
       "      <td>Closed</td>\n",
       "      <td>2026-12-25 20:03:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     year                date  \\\n",
       "0    2017      March 15, 2017   \n",
       "1    2017      March 14, 2017   \n",
       "115  2003  December 25 and 26   \n",
       "\n",
       "                                                notice          clean_date  \n",
       "0                                                 Open 2017-03-15 00:00:00  \n",
       "1    Open - 3 hours Delayed Arrival - With Option f... 2017-03-14 00:00:00  \n",
       "115                                             Closed 2026-12-25 20:03:00  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 2026?!\n",
    "opm[opm.clean_date > '2017']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# manually append Dec 25 and 26, 2003\n",
    "opm.loc[len(opm)]=[2003, 'December 25', 'Closed', pd.to_datetime('December 25, 2003')]\n",
    "opm.loc[len(opm)]=[2003, 'December 26', 'Closed', pd.to_datetime('December 26, 2003')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# drop that ugly other one\n",
    "opm.drop(opm.index[115], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# reindex\n",
    "opm.reindex(copy=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# check\n",
    "opm.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "opm.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean the notice texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# how many different notices have they used over the years?\n",
    "opm.notice.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# and what are they?\n",
    "opm.notice.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# when was the first time teleworking was allowed? This may explain closures v delays\n",
    "opm[opm['notice'].str.contains(\"Telework\")].clean_date.min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# when was the first time delay?\n",
    "opm[opm['notice'].str.contains(\"Delayed\")].clean_date.min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# how many closures?\n",
    "print(opm[opm.notice == 'Closed'].shape[0])\n",
    "opm[opm.notice == 'Closed']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# how many delays?\n",
    "print(opm[opm.notice.str.contains('hours delayed', case=False, regex=False)].shape[0])\n",
    "opm[opm.notice.str.contains('hours delayed', case=False, regex=False)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# get closure - like a convo with your ex\n",
    "def get_closures(text):\n",
    "    if 'Closed' in text:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# get delays - like an Amtrak\n",
    "def get_delays(text):\n",
    "    if 'hours Delayed' in text:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# get either - like...I dunno\n",
    "strings = (\"hours Delayed\", \"Closed\")\n",
    "def get_closures_delays(text):\n",
    "    if any(s in text for s in strings):\n",
    "        return 1\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# add columns for closures, delays, or either to dataset \n",
    "opm['closed']= opm.notice.apply(lambda text: get_closures(text))\n",
    "opm['delayed'] = opm.notice.apply(lambda text: get_delays(text))\n",
    "opm['delayed_or_closed'] = opm.notice.apply(lambda text: get_closures_delays(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "opm.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "opm_ts = opm.set_index('clean_date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# plot closure/delays over time\n",
    "fig, ax = plt.subplots()\n",
    "plt.suptitle(\"Federal Delays Due to Snow are Becoming More Popular\", size=16)\n",
    "plt.title(\"Federal Government Closures and Delays\", size=12)\n",
    "plt.xlabel('Time')\n",
    "ax.vlines(x=(opm_ts[opm_ts['closed'] == 1].index), ymin=0, ymax=1, color='red', linewidth=2, label='Closed')\n",
    "ax.vlines(x=(opm_ts[opm_ts['delayed'] == 1].index), ymin=0, ymax=1, color='orange', linewidth=2, label='Delayed')\n",
    "plt.xlim(['1999-06', '2017-10'])\n",
    "plt.yticks([])\n",
    "plt.ylabel('')\n",
    "plt.legend(loc='lower left')\n",
    "fig.set_size_inches(20, 6)\n",
    "plt.savefig('./graphs/closures-delays.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Google Search Volume Data\n",
    "\n",
    "Read in the data, make a few plots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# read in data\n",
    "search = pd.read_csv('./google-trends-dc-snow.csv')\n",
    "search.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# rename columns\n",
    "search.rename(columns={'Week':'week', 'snow: (District of Columbia)':'volume'}, inplace=True)\n",
    "search.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# format week column to datetime\n",
    "search['week'] =  pd.to_datetime(search['week'])\n",
    "# create timeseries\n",
    "search_ts = search.set_index('week')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# plot it\n",
    "search_ts.plot(color='dodgerblue', legend=None)\n",
    "plt.suptitle(\"Looking for Snow\", y=.99, size=16)\n",
    "plt.title(\"Google searches for 'Snow' in DC\", y=1.01, size=12)\n",
    "plt.xlim([search_ts.index.min(), '2017-04'])\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Search Volume')\n",
    "plt.savefig('./graphs/snow-searches.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# earliest search data?\n",
    "search_ts.index.min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# get federal data to match\n",
    "opm_ts_12 = opm_ts['2012-03-25':]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# plot snow searches vs snow closures\n",
    "fig, ax = plt.subplots()\n",
    "ax.vlines(x=(opm_ts_12[opm_ts_12['closed'] == 1].index), ymin=0, ymax=100, color='red', linestyles='dashed', linewidth=0.5, label='Closure')\n",
    "ax.plot(search_ts, color='dodgerblue')\n",
    "plt.xlim([search_ts.index.min(), '2017-04'])\n",
    "\n",
    "plt.suptitle(\"Looking for Snow\", y=.99, size=16)\n",
    "plt.title(\"Google searches for 'Snow' in DC vs Federal Government Closures\", y=1.01, size=12)\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel(\"Google 'Snow' Search Volume\")\n",
    "plt.legend(loc='best')\n",
    "#fig.set_size_inches(20, 6)\n",
    "plt.savefig('./graphs/searches-vs-closures.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "search.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Historical Weather Forecast Data\n",
    "\n",
    "Read in the data, make some plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "forecast = pd.read_csv('forecasts.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# rid unnecessary column...\n",
    "forecast.drop('Unnamed: 0', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "forecast.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# make date a datetime object\n",
    "forecast['date'] =  pd.to_datetime(forecast['date'])\n",
    "# create timeseries\n",
    "forecast_ts = forecast.set_index('date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "forecast_ts.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "forecast_ts.describe(include='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# DC, not a bad place to live\n",
    "forecast_ts.summary.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# plot \n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(forecast_ts.drop(['summary', 'temp', 'precip', 'precip_type','wind', 'visibility'], axis=1), color='dodgerblue', label='Precip Intensity')\n",
    "ax.vlines(x=(opm_ts_12[opm_ts_12['closed'] == 1].index), ymin=0, ymax=.5, color='red', linestyles='dashed', linewidth=0.5, label='Closure')\n",
    "plt.xlim([search_ts.index.min(), '2017-04'])\n",
    "\n",
    "plt.suptitle(\"Insensitive\", y=.99, size=16)\n",
    "plt.title(\"Forecasted Precipitation Intensity vs Federal Government Closures\", y=1.01, size=12)\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel(\"Precipitation Intensiy\")\n",
    "plt.legend(loc='best')\n",
    "plt.savefig('./graphs/precip_intensity_vs_closure.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# rain and snow PROBABILITY vs closure\n",
    "# thanks: http://people.duke.edu/~ccc14/pcfb/numpympl/MatplotlibBarPlots.html\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "# basics\n",
    "width=5\n",
    "x_rain = forecast_ts[forecast_ts.precip_type=='rain'].index\n",
    "y_rain = forecast_ts[forecast_ts.precip_type=='rain'].precip\n",
    "x_snow = forecast_ts[forecast_ts.precip_type=='snow'].index\n",
    "y_snow = forecast_ts[forecast_ts.precip_type=='snow'].precip\n",
    "\n",
    "## the bars\n",
    "rain = ax.bar(x_rain, y_rain, width,color='dodgerblue', label = 'Rain')\n",
    "snow = ax.bar(x_snow, y_snow, width, color='cyan', label = 'Snow')\n",
    "\n",
    "#closure\n",
    "ax.vlines(x=(opm_ts_12[opm_ts_12['closed'] == 1].index), ymin=0, ymax=1, color='red', linestyles='dashed', linewidth=1, label='Closure')\n",
    "\n",
    "# set axes, title, and legend\n",
    "ax.xaxis_date()\n",
    "plt.xlim([search_ts.index.min(), '2017-04'])\n",
    "plt.suptitle(\"Snow or Rain: The Government is (Usually) Open\", y=.99, size=16)\n",
    "plt.title(\"Forecasted Precipitation Probability vs Federal Government Closures\", y=1.01, size=12)\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel(\"Precipitation Probability\")\n",
    "plt.legend(loc='best')\n",
    "plt.savefig('./graphs/precip_chance_type_vs_closure.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# combine the above two\n",
    "# rain and snow INTENSITY vs closure\n",
    "# thanks: http://people.duke.edu/~ccc14/pcfb/numpympl/MatplotlibBarPlots.html\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "# basics\n",
    "width=5\n",
    "x_rain = forecast_ts[forecast_ts.precip_type=='rain'].index\n",
    "y_rain = forecast_ts[forecast_ts.precip_type=='rain'].precip_intensity\n",
    "x_snow = forecast_ts[forecast_ts.precip_type=='snow'].index\n",
    "y_snow = forecast_ts[forecast_ts.precip_type=='snow'].precip_intensity\n",
    "\n",
    "## the bars\n",
    "rain = ax.bar(x_rain, y_rain, width,color='dodgerblue', label = 'Rain')\n",
    "snow = ax.bar(x_snow, y_snow, width, color='cyan', label = 'Snow')\n",
    "\n",
    "#closure\n",
    "ax.vlines(x=(opm_ts_12[opm_ts_12['closed'] == 1].index), ymin=0, ymax=.5, color='red', linestyles='dashed', linewidth=1, label='Closure')\n",
    "\n",
    "# set axes, title, and legend\n",
    "ax.xaxis_date()\n",
    "plt.xlim([search_ts.index.min(), '2017-04'])\n",
    "plt.suptitle(\"Snow or Rain: The Government is (Usually) Open\", y=.99, size=16)\n",
    "plt.title(\"Forecasted Precipitation Intensity vs Federal Government Closures\", y=1.01, size=12)\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel(\"Precipitation Intensity\")\n",
    "plt.legend(loc='best')\n",
    "plt.savefig('./graphs/precip_intensity_type_vs_closure.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# temperature vs closure?\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(forecast_ts.temp, color='dodgerblue', label='Temperature')\n",
    "ax.vlines(x=(opm_ts_12[opm_ts_12['closed'] == 1].index), ymin=0, ymax=100, color='red', linestyles='dashed', linewidth=0.5, label='Closure')\n",
    "plt.xlim([search_ts.index.min(), '2017-04'])\n",
    "\n",
    "plt.suptitle(\"Cold in Here\", y=.99, size=16)\n",
    "plt.title(\"Forecasted Temperature vs Federal Government Closures\", y=1.01, size=12)\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel(\"Temperature\")\n",
    "plt.legend(loc='best')\n",
    "plt.savefig('./graphs/temperature_vs_closure.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# temperature 10 day rolling average vs closure?\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(forecast_ts.temp.rolling(window=10,center=False).mean(), color='dodgerblue', label='Temperature')\n",
    "ax.vlines(x=(opm_ts_12[opm_ts_12['closed'] == 1].index), ymin=0, ymax=100, color='red', linestyles='dashed', linewidth=0.5, label='Closure')\n",
    "plt.xlim([search_ts.index.min(), '2017-04'])\n",
    "\n",
    "plt.suptitle(\"No Chill\", y=.99, size=16)\n",
    "plt.title(\"Forecasted Temperature 10 Day Rolling Average vs Federal Government Closures\", y=1.01, size=12)\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel(\"Temperature\")\n",
    "plt.legend(loc='best')\n",
    "plt.savefig('./graphs/temperature10day_vs_closure.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# wind rolling averagevs closure?\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(forecast_ts.wind.rolling(window=10,center=False).mean(), color='dodgerblue', label='Wind')\n",
    "ax.vlines(x=(opm_ts_12[opm_ts_12['closed'] == 1].index), ymin=0, ymax=16, color='red', linestyles='dashed', linewidth=0.5, label='Closure')\n",
    "plt.xlim([search_ts.index.min(), '2017-04'])\n",
    "\n",
    "plt.suptitle(\"This Blows\", y=.99, size=16)\n",
    "plt.title(\"Forecasted Wind Speed 10 Day Rolling Average vs Federal Government Closures\", y=1.01, size=12)\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel(\"Wind Speed\")\n",
    "plt.legend(loc='best')\n",
    "plt.savefig('./graphs/wind10day_vs_closure.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# visibility vs closure - looks cluttered\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "# basics\n",
    "width=5\n",
    "x_vis = forecast_ts[forecast_ts.visibility != 10].index\n",
    "y_vis = forecast_ts[forecast_ts.visibility != 10].visibility\n",
    "\n",
    "## the bars\n",
    "ax.bar(x_vis, y_vis, width,color='dodgerblue', label = 'Visibility')\n",
    "\n",
    "#closure\n",
    "ax.vlines(x=(opm_ts_12[opm_ts_12['closed'] == 1].index), ymin=0, ymax=10, color='red', linestyles='dashed', linewidth=1, label='Closure')\n",
    "\n",
    "# set axes, title, and legend\n",
    "ax.xaxis_date()\n",
    "plt.xlim([search_ts.index.min(), '2017-04'])\n",
    "plt.suptitle(\"I Can See the Government is Open\", y=.99, size=16)\n",
    "plt.title(\"Forecasted Visibility vs Federal Government Closure\", y=1.01, size=12)\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel(\"Visibility in Miles\")\n",
    "plt.legend(loc='best')\n",
    "plt.savefig('./graphs/visibility_vs_closure.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# rain and snow vs closure\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "# basics\n",
    "width=5\n",
    "x_vis = forecast_ts.visibility.index\n",
    "y_vis = forecast_ts.visibility-10\n",
    "\n",
    "## the bars\n",
    "ax.bar(x_vis, y_vis, width,color='dodgerblue', label = 'Visibility')\n",
    "\n",
    "#closure\n",
    "ax.vlines(x=(opm_ts_12[opm_ts_12['closed'] == 1].index), ymin=0, ymax=-10, color='red', linestyles='dashed', linewidth=1, label='Closure')\n",
    "\n",
    "# set x axis\n",
    "ax.xaxis_date()\n",
    "plt.xlim([search_ts.index.min(), '2017-04'])\n",
    "\n",
    "# set y axis\n",
    "labels = [0,2,4,6,8,10]\n",
    "ax.set_yticklabels(labels)\n",
    "\n",
    "# title and legend\n",
    "plt.suptitle(\"I Can See the Government is Open\", y=.99, size=16)\n",
    "plt.title(\"Forecasted Visibility vs Federal Government Closure\", y=1.01, size=12)\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel(\"Visibility in Miles (Inverted)\")\n",
    "plt.legend(loc='best')\n",
    "plt.savefig('./graphs/visibility_inverted_vs_closure.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Prep\n",
    "\n",
    "What is the most indicative feature of a federal government closure? Let's get our features ready.\n",
    "\n",
    "\n",
    "**ASSUMPTIONS/LIMITATIONS**\n",
    "\n",
    "- Because our Google search volume data is only available at the weekly level, we have no choice but to model at that unit of observation. This is clearly problematic: closure decisions are not made within the *week* of a forecast, but are instead made the day of. However, it is the best we can do.\n",
    "\n",
    "- Second, because we only have forecast and search volume data going back to March 2012, we can only model the closures and/or delays that took place from March 2012 forward.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# minimum and maximum dates?\n",
    "print('Minimum forecast history', forecast.date.min(), 'Max forecast history', forecast.date.max())\n",
    "print('Minimum search history', search.week.min(), 'Max search history', search.week.max())\n",
    "print('Minimum OPM history', opm.clean_date.min(), 'Max OPM history', opm.clean_date.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# create df that has our desire date range based on min/max of search data\n",
    "dates = pd.Series(pd.date_range('2012-03-25', '2017-03-12'))\n",
    "\n",
    "# create df with that Series - \"clean date\" because we'll merge OPM first\n",
    "snow_fun = pd.DataFrame(dates, columns=['clean_date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "snow_fun.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# add closures, delays, and both to df\n",
    "snow_fun = pd.merge(snow_fun,opm[['clean_date','closed', 'delayed', 'delayed_or_closed']],on='clean_date', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# check 'er out\n",
    "snow_fun.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create a week column that is also just the clean date to merge search data\n",
    "snow_fun['week'] = snow_fun['clean_date']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# merge search data\n",
    "snow_fun = pd.merge(snow_fun,search[['week', 'volume']],on='week', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "snow_fun.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# time to add historical forecast data! We need a date column\n",
    "snow_fun['date'] = snow_fun['clean_date']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# merge weather data\n",
    "snow_fun = pd.merge(snow_fun,forecast,on='date', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# check out out new df\n",
    "snow_fun.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# set index as date\n",
    "snow_fun.set_index('date', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# no longer need our join keys\n",
    "snow_fun.drop(['clean_date', 'week'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# make datetime object\n",
    "snow_fun.index = pd.to_datetime(snow_fun.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# make a feature for months\n",
    "snow_fun['month'] = snow_fun.index.month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "snow_fun.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# prepare data dataframe by week given stated assumptions above - average these features\n",
    "# thanks http://stackoverflow.com/questions/24082784/pandas-dataframe-groupby-datetime-month\n",
    "averages = ['volume', 'temp', 'precip', 'precip_intensity', 'precip_type', 'wind', 'visibility']\n",
    "X = pd.DataFrame(snow_fun[averages].groupby(pd.TimeGrouper(freq='W')).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# add months to this df - do not average month across weeks\n",
    "X['month'] = snow_fun.groupby(pd.TimeGrouper(freq='W')).month.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# our y is equal to 1 if there was a closure in a given week else it's zero\n",
    "y = [1 if val == 1 else 0 for val in snow_fun.groupby(pd.TimeGrouper(freq='W')).closed.max().values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# does it all check out? (Yes. It's go time.)\n",
    "print(X.shape)\n",
    "print(len(y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Building\n",
    "\n",
    "First, I'll use a logistic regression to see classify whether a given week will include a closure. I'll use both sklearn and statsmodels.\n",
    "\n",
    "I'm also going to use a random forest to classify these estimates. They're traditionally strong ensembling models, and they will enable me to evaluate feature importances. In addition, random forest are known to even rival ARIMA models when handling time data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rf_grid = RandomForestClassifier(random_state=99, n_jobs=50)\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.cross_validation import cross_val_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import precision_score\n",
    "\n",
    "import statsmodels.formula.api as sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# create generic data df using prepped data above\n",
    "data = X\n",
    "data['target'] = y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# important note: severely imbalanced classes :(\n",
    "data.target.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# train, test, split and use stratification\n",
    "X_train, X_test, y_train, y_test = train_test_split(data.drop('target', axis=1), data['target'], \n",
    "                                                    test_size=0.33, stratify=data['target'].values, random_state=75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# calculate null accuracy\n",
    "y_test_binary = np.where(y_test==0, 1, 0)\n",
    "max(y_test_binary.mean(), 1 - y_test_binary.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# using a plain logistic regression predicts open every day - our null accuracy (or worse)\n",
    "logreg = LogisticRegression()\n",
    "logreg.fit(X_train, y_train)\n",
    "y_pred = logreg.predict(X_test)\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "print acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# set up a different model to grid search\n",
    "logreg = LogisticRegression()\n",
    "c_range = 10.**np.arange(-2, 3)\n",
    "penalties = ['l1','l2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# gridsearch across options\n",
    "# optimize for log loss: http://www.exegetic.biz/blog/2015/12/making-sense-logarithmic-loss/\n",
    "gs = GridSearchCV(logreg, {'penalty':penalties, 'C':c_range}, verbose=True, cv=5, scoring='log_loss')\n",
    "gs.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# best parameters?\n",
    "gs.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# use these params\n",
    "logreg = LogisticRegression(penalty = 'l1', C = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# fit it\n",
    "logreg.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# predict with these params\n",
    "y_pred = logreg.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# roc_auc?\n",
    "roc_auc_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# accuracy?\n",
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# coefficients?\n",
    "logreg.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# get a report in statsmodels using regularization\n",
    "model = sm.logit(\"target ~ volume + temp + precip + precip_intensity + wind + visibility + month\", data=data).fit_regularized(method='l1', alpha=0.01)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# fit a regular logistic regression in statsmodels just to see variable importances\n",
    "model = sm.logit(\"target ~ volume + temp + precip + precip_intensity + wind + visibility + month\", data=data).fit()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interaction Terms\n",
    "\n",
    "Temp x precip, temp x precip_intensity, temp x volume, wind x precip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create interaction terms\n",
    "X['temp_precip'] = X['temp'] * X['precip']\n",
    "X['temp_precip_intensity'] = X['temp'] * X['precip_intensity']\n",
    "X['temp_volume'] = X['temp'] * X['volume']\n",
    "X['wind_precip'] = X['wind'] * X['precip']\n",
    "X['wind_precip_intensity'] = X['wind'] * X['precip_intensity']\n",
    "X['wind_volume'] = X['wind'] * X['volume']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = sm.logit(\"target ~ volume + temp + precip + precip_intensity + wind + visibility + month + temp_precip + temp_precip_intensity + temp_volume + wind_precip + wind_precip_intensity + wind_volume\", data=data).fit()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With all interaction terms available, it appears precipitation is *only* signifiant when wind is also anticipated. Wind and preciptation together represent a significant factor, but wind alone is not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aside from the interactions, it appears we only need to use the forecasted average weekly temperature to determine with a high degree accuracy whether OPM is going to close the federal government or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# non-interaction features\n",
    "features = ['volume', 'temp', 'precip', 'precip_intensity', 'wind', 'visibility', 'month']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# gridsearch across options for PRECISION - only predicting snow days when they happen\n",
    "# what if I optimize for precision?\n",
    "gs = GridSearchCV(logreg, {'penalty':penalties, 'C':c_range}, verbose=True, cv=5, scoring='precision')\n",
    "gs.fit(X[features], y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# best parameters?\n",
    "gs.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# use these params\n",
    "logreg = LogisticRegression(penalty = 'l1', C = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# fit it\n",
    "logreg.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# precision?\n",
    "precision_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# roc_auc?\n",
    "roc_auc_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# accuracy?\n",
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# coefficients?\n",
    "coefs = pd.DataFrame(logreg.coef_, columns = features)\n",
    "coefs.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Parameter dictionary of settings options for the model we're passing grid search.\n",
    "param_grid = {\n",
    "    'criterion': ['gini', 'entropy'],\n",
    "    'max_depth' : [None,2,5,8],\n",
    "    'max_features' : ['auto', 'sqrt', 'log2'],\n",
    "    'class_weight' : ['balanced', None]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Instantiate grid search, score on roc_auc\n",
    "grid = GridSearchCV(rf_grid, param_grid, cv=5, scoring='roc_auc')\n",
    "\n",
    "# Overfit the grid search to X and y.\n",
    "grid.fit(X[features], y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Store the best parameters in a variable.\n",
    "params = grid.best_params_\n",
    "print \"Best score =\", grid.best_score_\n",
    "print params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Run Random Forest on the data.\n",
    "rf = RandomForestClassifier(**params)\n",
    "rf_model = rf.fit(X_train, y_train)\n",
    "y_pred = rf.predict(X_test)\n",
    "\n",
    "print \"RandomForest Cross_Val Score:\\t\", cross_val_score(rf, X_train, y_train, cv=5).mean()\n",
    "print \"Train/Test RandomForest Score:\\t\", rf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# create dataframe showing feature importances of rf\n",
    "df_features = pd.DataFrame(columns=['Features', 'Importance (Gini Index)'])\n",
    "df_features['Features'] = columns=features\n",
    "df_features['Importance (Gini Index)'] = rf.feature_importances_\n",
    "df_features.sort_values('Importance (Gini Index)', ascending=False, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_features.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# feature importances determining closure\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ind = np.arange(len(features))\n",
    "\n",
    "# basics\n",
    "width=0.5\n",
    "x = ind\n",
    "y = df_features['Importance (Gini Index)'].values\n",
    "\n",
    "## the bars\n",
    "ax.bar(x, y, width,color='dodgerblue', label = 'Importance (Gini Index)')\n",
    "\n",
    "# handle axes and ticks\n",
    "ax.set_ylabel('Gini Index')\n",
    "ax.set_xticks(ind + width / 2)\n",
    "plt.xlabel('Factor')\n",
    "ax.set_xticklabels(('Temperature', 'Wind', 'Visibility', \"Google Searches \\nfor 'Snow'\", 'Month', 'Precipitation \\nIntensity', 'Precipitation'))\n",
    "\n",
    "# title, size, and legend\n",
    "plt.suptitle(\"Colds Weather Determines Federal Closures\", y=.99, size=16)\n",
    "plt.title(\"The Most Important Factors in OPM's Decisions to Close the Federal Government\", y=1.01, size=12)\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "plt.legend(loc='best')\n",
    "plt.savefig('./graphs/rf_feature_importances.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our logistic regressions and random forests confirm: OPM most relies on temperature in their decision to close the federal government. In fact, we can reject at the 5% significance level that Google searches for \"Snow\" play a factor in their decision-making.\n",
    "\n",
    "Nonetheless, even our regularized models kept Google searches for snow as a term. Keep Googling for that Snow, DC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
